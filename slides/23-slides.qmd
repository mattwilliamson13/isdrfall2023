---
title: "Statistical Modelling II"
subtitle: "HES 505 Fall 2023: Session 23"
author: "Matt Williamson"
execute: 
  eval: true
format: 
  revealjs:
    theme: mytheme.scss
    slide-number: true
    show-slide-number: print
    self-contained: true  
---

```{r}
#| include: false
library(sf)
library(tidyverse)
library(maps)
library(tmap)
library(terra)
library(corrplot)
```

# Objectives {background="#0033A0"}

By the end of today you should be able to:

- Articulate the differences between statisitical learning classifiers and logistic regression

- Describe several classification trees and their relationship to Random Forests

- Describe MaxEnt models for presence-only data

# Revisiting Classification {background="#0033A0"}

## Favorability in General

$$
\begin{equation}
F(\mathbf{s}) = f(w_1X_1(\mathbf{s}), w_2X_2(\mathbf{s}), w_3X_3(\mathbf{s}), ..., w_mX_m(\mathbf{s}))
\end{equation}
$$

* Logistic regression treats $f(x)$ as a (generalized) linear function

* Allows for multiple qualitative classes

* Ensures that estimates of $F(\mathbf{s})$ are [0,1] 

## Key assumptions of logistic regression

* Dependent variable must be binary

* Observations must be independent (important for spatial analyses)

* Predictors should not be collinear

* Predictors should be linearly related to the log-odds

* __Sample Size__


## Beyond Linearity

* Logistic (and other generalized linear models) are relatively interpretable

* Probability theory allows robust inference of effects

* Predictive power can be low

* Relaxing the linearity assumption can help

## Classification Trees

* Use decision rules to segment the predictor space

* Series of consecutive decision rules form a 'tree'

* Terminal nodes (leaves) are the outcome; internal nodes (branches) the splits


## Classification Trees

* Divide the predictor space ($R$) into $J$ non-overlapping regions

* Every observation in $R_j$ gets the same prediction

* _Recursive binary splitting_

* Pruning and over-fitting

## An Example

Inputs from the `dismo` package

```{r}
base.path <- "/Users/mattwilliamson/Google Drive/My Drive/TEACHING/Intro_Spatial_Data_R/Data/2021/session28/" #sets the path to the root directory

pres.abs <- st_read(paste0(base.path, "presenceabsence.shp"), quiet = TRUE) #read the points with presence absence data
## OGR data source with driver: ESRI Shapefile 
## Source: "/Users/matthewwilliamson/Downloads/session28/presenceabsence.shp", layer: "presenceabsence"
## with 100 features
## It has 1 fields
pred.files <- list.files(base.path,pattern='grd$', full.names=TRUE) #get the bioclim data

pred.stack <- rast(pred.files) #read into a RasterStack
names(pred.stack) <- c("MeanAnnTemp", "TotalPrecip", "PrecipWetQuarter", "PrecipDryQuarter", "MinTempCold", "TempRange")
plot(pred.stack)
```

## An Example

The sample data

::: columns
::: {.column width="40%"}
```{r}
#| echo: true
head(pres.abs)
```
:::
::: {.column width="60%"}
```{r}
#| fig-width: 6
#| fig-height: 6
plot(pred.stack[[1]])
pres.pts <- pres.abs[pres.abs$y == 1,]
abs.pts <- pres.abs[pres.abs$y == 0,]
plot(pres.pts$geometry, add=TRUE, pch=3, col="blue")
plot(abs.pts$geometry, add=TRUE, pch ="-", col="red")
```
:::
:::

## An Example

Building our dataframe


```{r}
#| echo: true
pts.df <- terra::extract(pred.stack, vect(pres.abs), df=TRUE)
head(pts.df)
```

## An Example

Building our dataframe

```{r}
#| echo: true
pts.df[,2:7] <- scale(pts.df[,2:7])
summary(pts.df)
```

## An example

* Fitting the classification tree

```{r}
#| echo: true

library(tree)
pts.df <- cbind(pts.df, pres.abs$y)
colnames(pts.df)[8] <- "y"
pts.df$y <- as.factor(ifelse(pts.df$y == 1, "Yes", "No"))
tree.model <- tree(y ~ . , pts.df)
plot(tree.model)
text(tree.model, pretty=0)
```



## An example

* Fitting the classification tree

```{r}
#| echo: true
summary(tree.model)
```


## Benefits and drawbacks

::: columns
::: {.column width="50%"}
**Benefits**

* Easy to explain

* Links to human decision-making

* Graphical displays

* Easy handling of qualitative predictors

:::
::: {.column width="50%"}
**Costs**

* Lower predictive accuracy than other methods

* Not necessarily robust

:::
:::

## Random Forests

::: columns
::: {.column width="50%"}
::: {style="font-size: 0.7em"}
* Grow 100(000s) of trees using bootstrapping

* Random sample of predictors considered at each split

* Avoids correlation amongst multiple predictions

* Average of trees improves overall outcome (usually)

* Lots of extensions

:::
:::
::: {.column width="50%"}
![](img/slide_20/randomforest.png)
:::
:::

## An example

* Fitting the Random Forest



```{r}
#| echo: true
library(randomForest)
class.model <- y ~ .
rf2 <- randomForest(class.model, data=pts.df)
varImpPlot(rf2)

```

# Modelling Presence-Background Data {background="#9F281A"}

## The sampling situation

::: columns
::: {.column width="40%"}
![From [Lentz et al. 2008](https://www.journals.uchicago.edu/doi/full/10.1086/528754)](img/slide_19/maxentresult.png)
:::
::: {.column width="60%"}

* Opportunistic collection of presences only

* Hypothesized predictors of occurrence are measured (or extracted) at each presence

* Background points (or pseudoabsences) generated for comparison

:::
:::

## The Challenge with Background Points

* What constitutes background?

* Not measuring _probability_, but relative likelihood of occurrence

* Sampling bias affects estimation

* The intercept

$$ 
\begin{equation}
y_{i} \sim \text{Bern}(p_i)\\
\text{link}(p_i) = \mathbf{x_i}'\beta + \alpha
\end{equation}
$$

## MaxEnt

::: columns
::: {.column width="40%"}

![From [Lentz et al. 2008](https://www.journals.uchicago.edu/doi/full/10.1086/528754)](img/slide_20/maxentresult.png)

:::
::: {.column width="60%"}

* Opportunistic collection of presences only

* Hypothesized predictors of occurrence are measured (or extracted) at each presence

* Background points (or pseudoabsences) generated for comparison

:::
:::


## Maximum Entropy models

::: columns 
::: {.column width="60%"}
::: {style="font-size: 0.7em"}
* MaxEnt (after the original software)

* Need _plausible_ background points across the remainder of the study area

* Iterative fitting to maximize the distance between predictions generated by a spatially uniform model 

* Tuning parameters to account for differences in sampling effort, placement of background points, etc

* Development of the model beyond the scope of this course, but see [Elith et al. 2010](https://web.stanford.edu/~hastie/Papers/maxent_explained.pdf)

:::
:::
::: {.column width="40%"}
![From [Elith et al. 2010](https://web.stanford.edu/~hastie/Papers/maxent_explained.pdf)](img/slide_20/maxentschem.png)
:::
:::


## Challenges with MaxEnt

* Not measuring _probability_, but relative likelihood of occurrence

* Sampling bias affects estimation (but can be mitigated using tuning parameters)

* Theoretical issues with background points and the intercept

* Recent developments relate MaxEnt (with cloglog links) to Inhomogenous Point Process models

## Extensions

* Polynomial, splines, piece-wise regression

* Neural nets, Support Vector Machines, many many more




